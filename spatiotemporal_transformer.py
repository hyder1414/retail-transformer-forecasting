# -*- coding: utf-8 -*-
"""spatiotemporal_transformer.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GvfVGkkpyETiKRqmKh8Nzyt4JD_z7oJC
"""

import math
from dataclasses import dataclass
from typing import Dict, List

import torch
import torch.nn as nn


class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(dropout)

        pe = torch.zeros(max_len, d_model)
        pos = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)
        div = torch.exp(
            torch.arange(0, d_model, 2, dtype=torch.float32)
            * (-math.log(10000.0) / d_model)
        )
        pe[:, 0::2] = torch.sin(pos * div)
        pe[:, 1::2] = torch.cos(pos * div)
        pe = pe.unsqueeze(0)  # (1, L, D)

        self.register_buffer("pe", pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B, L, D)
        x = x + self.pe[:, : x.size(1), :]
        return self.dropout(x)


@dataclass
class SpatioTemporalConfig:
    enc_input_length: int = 30
    dec_output_length: int = 4

    enc_feature_dim: int = 11   # overridden at runtime
    dec_feature_dim: int = 10   # overridden at runtime

    d_model: int = 64
    nhead: int = 4
    num_encoder_layers: int = 2
    num_decoder_layers: int = 2
    dim_feedforward: int = 128
    dropout: float = 0.1

    static_embed_dim: int = 16


class SpatioTemporalTransformer(nn.Module):
    """
    Encoderâ€“decoder Transformer with static Store/Dept/Type embeddings.
    """

    def __init__(
        self,
        cfg: SpatioTemporalConfig,
        static_vocab_sizes: Dict[str, int],
        static_cols_order: List[str],
    ):
        super().__init__()
        self.cfg = cfg
        self.static_cols_order = static_cols_order

        # Static embeddings per categorical static feature
        self.static_embeddings = nn.ModuleDict()
        for col, vocab_size in static_vocab_sizes.items():
            self.static_embeddings[col] = nn.Embedding(
                num_embeddings=vocab_size,
                embedding_dim=cfg.static_embed_dim,
            )

        total_static_dim = cfg.static_embed_dim * len(static_vocab_sizes)
        self.static_proj = nn.Linear(total_static_dim, cfg.d_model)

        # Project dynamic + static into model dimension
        self.enc_proj = nn.Linear(cfg.enc_feature_dim + cfg.d_model, cfg.d_model)
        self.dec_proj = nn.Linear(cfg.dec_feature_dim + cfg.d_model, cfg.d_model)

        self.enc_pos = PositionalEncoding(cfg.d_model, cfg.dropout)
        self.dec_pos = PositionalEncoding(cfg.d_model, cfg.dropout)

        self.transformer = nn.Transformer(
            d_model=cfg.d_model,
            nhead=cfg.nhead,
            num_encoder_layers=cfg.num_encoder_layers,
            num_decoder_layers=cfg.num_decoder_layers,
            dim_feedforward=cfg.dim_feedforward,
            dropout=cfg.dropout,
            batch_first=True,
        )

        self.head = nn.Linear(cfg.d_model, 1)

    def _static_repr(self, static_idx: torch.Tensor) -> torch.Tensor:
        # static_idx: (B, num_static)
        embs = []
        for i, col in enumerate(self.static_cols_order):
            emb_layer = self.static_embeddings[col]
            emb = emb_layer(static_idx[:, i])  # (B, static_embed_dim)
            embs.append(emb)
        static_concat = torch.cat(embs, dim=-1)  # (B, total_static_dim)
        static_repr = self.static_proj(static_concat)  # (B, d_model)
        return static_repr

    def forward(
        self,
        enc_x: torch.Tensor,       # (B, L_enc, F_enc)
        dec_x: torch.Tensor,       # (B, L_dec, F_dec)
        static_idx: torch.Tensor,  # (B, num_static)
    ) -> torch.Tensor:
        B, L_enc, _ = enc_x.shape
        _, L_dec, _ = dec_x.shape
        device = enc_x.device

        static_vec = self._static_repr(static_idx)           # (B, d_model)
        static_enc = static_vec.unsqueeze(1).expand(B, L_enc, -1)
        static_dec = static_vec.unsqueeze(1).expand(B, L_dec, -1)

        enc_in = torch.cat([enc_x, static_enc], dim=-1)
        dec_in = torch.cat([dec_x, static_dec], dim=-1)

        enc_in = self.enc_proj(enc_in)
        dec_in = self.dec_proj(dec_in)

        enc_in = self.enc_pos(enc_in)
        dec_in = self.dec_pos(dec_in)

        tgt_mask = nn.Transformer.generate_square_subsequent_mask(L_dec).to(device)

        out = self.transformer(enc_in, dec_in, tgt_mask=tgt_mask)  # (B, L_dec, D)
        out = self.head(out).squeeze(-1)                           # (B, L_dec)
        return out